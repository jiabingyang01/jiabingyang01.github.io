<!DOCTYPE html>
<html lang="en-US">
  <head>
    <title>Jiabing Yang's Homepage</title>

    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <meta name="description" content="Jiabing Yang">
    <meta name="keywords" content="yaoyao liu, liu yaoyao, yaoyaoliu, liuyaoyao">

    <meta property="og:type" content="website">
    <meta property="og:title" content="Jiabing Yang | Institute of Automation, Chinese Academy of Sciences">
    <meta property="og:url" content="https://www.cs.jhu.edu/~yyliu/">
    <meta property="og:site_name" content="Jiabing Yang | Institute of Automation, Chinese Academy of Sciences">
    <meta property="og:description" content="Yaoyao Liu is a postdoctoral fellow at Johns Hopkins University. His research lies in meta-learning, incremental learning, and image generation.">
    <meta property="og:locale" content="default">
    <meta property="og:image" content="./assets/img/avatar.jpg">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="Jiabing Yang | Institute of Automation, Chinese Academy of Sciences">
    <meta name="twitter:description" content="Yaoyao Liu is a postdoctoral fellow at Johns Hopkins University. His research lies in meta-learning, incremental learning, and image generation.">
    <meta name="twitter:image" content="./assets/img/avatar.jpg">

    <link rel="canonical" href="https://www.cs.jhu.edu/~yyliu/"/>
    <link rel="icon" media="(prefers-color-scheme:dark)" href="./assets/img/yjb.jpg" type="image/png" />
    <link rel="icon" media="(prefers-color-scheme:light)" href="./assets/img/yjb.jpg" type="image/png" />
    <link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin=anonymous>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css" integrity="sha512-z3gLpd7yknf1YoNbCzqRKc4qyor8gaKU1qmn+CShxbuBusANI9QpRohGBreCFkKxLhei6S9CQXFEbbKuqLg0DA==" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <link rel="stylesheet" href="./assets/css/style.css">
    <link rel="stylesheet" href="./assets/css/pub.css">
    <link rel="stylesheet" href="./assets/css/nav.css">
    
    <script src="./assets/js/github-stars.js"></script>
    <script type="text/javascript" src="./assets/js/jquery.js"></script>
    <script src="./assets/js/favicon-switcher.js" type="application/javascript"></script>
    
    <script type="text/javascript">
      function toggle_vis(id) {
          var e = document.getElementById(id);
          if (e.style.display == 'none')
              e.style.display = 'inline';
          else
              e.style.display = 'none';
      }
   </script>

   <script>
      function myFunction() {
      var x = document.getElementById("myLinks");
      if (x.style.display === "block") {
        x.style.display = "none";
      } else {
        x.style.display = "block";
      }
      }
   </script>
   

  </head>
  <body>
    
    <div class="topnav">

      <a href="https://www.ia.cas.cn/"><img width="190" src="./assets/img/ucas.png"></a>

      <div id="myLinks">
      


      </div>

      <a href="javascript:void(0);" class="icon" onclick="myFunction()">
        <i class="fa fa-bars"></i>
      </a>
      
    </div>
    

    <script src="./assets/js/vanilla-back-to-top.min.js"></script>
    <script>addBackToTop({
        backgroundColor: '#fff',
        innerHTML: 'Back to Top',
        textColor: '#333'
      })
    </script>
    <style>
        #back-to-top {
          border: 1px solid #ccc;
          border-radius: 0;
          font-size: 15px;
          width: 100px;
          text-align: center;
          line-height: 30px;
          height: 30px;
        }
    </style>
    
    <div class="wrapper">
      <header>
        
        
        <a class="image avatar"><img src="./assets/img/avatar.jpg" alt="avatar" /></a>
        

        <h1>Jiabing Yang</h1>

        
        <position style="font-size:1.10rem;">Artificial Intelligence PhD</position>
        <br>
        
        
        <a href="https://www.ia.cas.cn/" rel="noopener">Institute of Automation, Chinese Academy of Sciences</a>
        <br>
        
        
        <a href="" rel="noopener" target="_blank">
          <autocolor>Beijing, China</autocolor>
        </a>
        <br>
                
        
        <email><a href="mailto:jiabingyang01@gmail.com">jiabingyang01@gmail.com</a></email>
        

        <br>
        <br>

        

        
        <div class="social-icons">
        
        <a style="margin: 0 5px 0 0" href="https://scholar.google.com/">
          <i class="ai ai-google-scholar" style="font-size:1.2rem"></i>
        </a>  
        

        

        
        <a style="margin: 0 5px 0 0" href="https://github.com/jiabingyang01/">
          <i class="fab fa-github"></i>
        </a>
        

        

        
        </div>
        
        <br>

      </header>
      <section>

      <h1 id="about-me"></h1>

<h2 style="margin: 60px 0px 10px;">About Me</h2>

<p>Iâ€™m an undergraduate student majoring in Artificial Intelligence at <a href="https://www.yingcai.uestc.edu.cn/index.htm">Yingcai Honors College</a> of <a href="https://www.uestc.edu.cn/">University of Electronic Science and Technology of China</a>.</p>

<h1 id="research-interests"></h1>

<h2 style="margin: 30px 0px 10px;">Research Interests</h2>

<ul>
  <li><strong>Large Language Models:</strong> Control Text Generation (CTG), Agents.</li>
  <li><strong>Multimodal Large Language Models:</strong> Alignment, Hallucinations, Efficient Inference.</li>
  <li><strong>Embodied AI:</strong> Vision Language Action (VLA) Models.</li>
</ul>

<!--
<strong style="color:#e74d3c; font-weight:600"><strong style="color:#e74d3c; font-weight:600">I am currently on the 2023-2024 academic job market, looking for faculty positions in CS, CSE, ECE, IEOR, etc., related to Artificial Intelligence, Computer Vision, and Machine Learning. Please feel free to contact me if you are interested. I am also happy to give talks on my research in related seminars.</strong></strong>
-->

<h1 id="news"></h1>

<h2 style="margin: 30px 0px 10px;">News</h2>

<div class="news-section">
<ul>
<!--   <li><strong>[Aug. 2024]</strong> ðŸŽ‰ðŸŽ‰ðŸŽ‰Our paper about union contrastive decoding to mitigate hallucinations in LVLMs is submitted to AAAI 2025.</li> -->
  <li><strong>[Jul. 2025]</strong> ðŸŽ‰ðŸŽ‰ðŸŽ‰Our paper "EC-Flow: Enabling Versatile Robotic Manipulation from Action-Unlabeled Videos via Embodiment-Centric Flow" is accepted by <strong>ICCV 2025.</strong></li>
    <!-- <li><strong>[Jul. 2024]</strong> ðŸŽ‰ðŸŽ‰ðŸŽ‰I joined Westlake University as an Assistant Professor (PI) and established AGI lab.</li>  
  <li><strong>[Jul. 2024]</strong> Three papers are accepted to <a href="https://eccv.ecva.net/">ECCV 2024</a>.</li>  
    <li><strong>[June 2024]</strong> Our work on  <a href="https://arxiv.org/abs/2310.11284">self-supervised 3D scene flow estimation</a> is accepted by TPAMI.</li>
  <li><strong>[June 2024]</strong> ðŸ”¥We present <a href="https://buaacyw.github.io/mesh-anything/">MeshAnything</a>, a study on high-quality mesh generation with autoregressive transformers.</li>
  <li><strong>[May 2024]</strong> <a href="https://icoz69.github.io/stablellava-official/">StableLLaVA</a> is accepted by ACL 2024.</li>
  <li><strong>[April 2024]</strong> ðŸš€We introduce <a href="https://github.com/YvanYin/Metric3D">Metric3D V2</a>, the most capable monocular geometry foundation model for depth and normals estimation. Training codes and demos are available!</li>
  <li><strong>[Mar 2024]</strong> ðŸš€We introduce <a href="https://deaddawn.github.io/MovieLLM/">MovieLLM</a>, a long-video understanding multimodal LLM.</li>
  <li><strong>[Feb 2024]</strong> <a href="https://buaacyw.github.io/gaussian-editor/">GaussianEditor</a> and <a href="https://arxiv.org/abs/2311.18651">LL3DA</a> are accepted by CVPR2024.</li>
  <li><strong>[Dec 2023]</strong> ðŸš€ðŸš€ðŸš€We introduce <a href="https://appagent-official.github.io/">AppAgent</a>, a multimodal agent for operating smartphone apps.</li>
  <li><strong>[Dec 2023]</strong> <a href="https://github.com/buaacyw/IT3D-text-to-3D">IT3D</a> is accepted by AAAI 2024.</li>
  <li><strong>[Dec 2023]</strong> We presented <a href="https://icoz69.github.io/facestudio/">FaceStudio</a>, a powerful identity-preserving image synthesis model.</li>
  <li><strong>[Nov 2023]</strong> We presented <a href="https://shapegpt.github.io/">ShapeGPT</a>, a multimodal LLM for 3D shape generation.</li>
  <li><strong>[Nov 2023]</strong> <a href="https://tingxueronghua.github.io/ChartLlama/">ChartLlama</a> is released! It is a powerful LLM for chart understanding and generation.</li>
  <li><strong>[Nov 2023]</strong> We presented <a href="https://buaacyw.github.io/gaussian-editor/">GaussianEditor</a>, a powerful 3D editing algorithm.</li>
  <li><strong>[Oct 2023]</strong> Pleased to be recognized among <a href="https://elsevier.digitalcommonsdata.com/datasets/btchxktzyw/6">2023 Top 2% Scientists by Stanford University</a>.</li>
  <li><strong>[Sept 2023]</strong> We presented <a href="https://arxiv.org/abs/2309.09724">Robust Depth</a> for robust geometry-preserving zero-shot depth estimation, which is accepted by ICCV 2023.</li>
  <li><strong>[Aug 2023]</strong> We presented <a href="https://github.com/buaacyw/IT3D-text-to-3D">IT3D</a>, a plug-and-play to improve the results of 3D AIGC models.</li>
  <li><strong>[Aug 2023]</strong> We have released <a href="https://icoz69.github.io/stablellava-official/">StableLLaVA</a>, a clever strategy for collecting datasets to train multimodal LLMs.</li>
  <li><strong>[Jul. 2023]</strong> Our work, <a href="https://arxiv.org/abs/2307.10984">Metric3D</a>, accepted by ICCV 2023, won first place in the <a href="https://jspenmar.github.io/MDEC/">2nd Monocular Depth Estimation Competition at CVPR</a>.</li>
  <li><strong>[Jul. 2023]</strong> Three papers are accepted to ICCV 2023.</li>
  <li><strong>[May 2023]</strong> We have released <a href="https://arxiv.org/abs/2305.19012">StyleAvatar3D</a>, a work for 3D stylized avatar generation.</li> -->
</ul>
</div>

<!-- <ul>
<li><strong>[Jul. 2024]</strong> Our paper about <a href="https://www.cs.jhu.edu/~yyliu/preprints/iNeMo_Incremental_Neural_Mesh_Models_for_Robust_Class-Incremental_Learning.pdf">robust class-incremental learning</a> is accepted to <a href="https://eccv2024.ecva.net/">ECCV 2024</a>.</li>
<li><strong>[Apr. 2024]</strong> I will serve as an Area Chair of <a href="https://neurips.cc/Conferences/2024">NeurIPS 2024</a> and <a href="https://2024.acmmm.org/">ACM MM 2024</a>.</li>
<li><strong>[Jan. 2024]</strong> Our paper about <a href="https://arxiv.org/pdf/2306.08103.pdf">diffusion models</a> is accepted to <a href="https://iclr.cc/Conferences/2024/">ICLR 2024</a>.</li>
<li><strong>[Dec. 2023]</strong> I will serve as an Area Chair of <a href="https://eccv2024.ecva.net/">ECCV 2024</a> and <a href="https://www.auai.org/uai2024/">UAI 2024</a>.</li>
<li><strong>[Oct. 2023]</strong> I gave a talk on continual learning at <a href="https://bair.berkeley.edu/">BAIR, UC Berkeley</a>. </li>
<li><strong>[Aug. 2023]</strong> I will serve as an Area Chair of <a href="https://cvpr.thecvf.com/">CVPR 2024</a>, <a href="https://iclr.cc/">ICLR 2024</a>, and <a href="https://aistats.org/aistats2024/">AISTATS 2024</a>.</li>
<li><strong>[Aug. 2023]</strong> I will give a talk at <a href="https://engineering.purdue.edu/ChanGroup/comp_imaging_seminar.html">Purdue Computational Imaging Seminar</a>.</li>
<li><strong>[May 2023]</strong> Our paper about <a href="https://arxiv.org/pdf/2306.00988.pdf">continual learning</a> is accepted to <a href="https://conferences.miccai.org/2023/en/">MICCAI 2023</a>.</li>
<li><strong>[Apr. 2023]</strong> I will give a talk on continual learning at <a href="https://calendars.illinois.edu/detail/2568?eventId=33456212">UIUC External Speaker Series</a>.</li>
<li><strong>[Apr. 2023]</strong> I will give a talk on continual learning at <a href="https://sites.google.com/view/visionseminar">MIT Vision and Graphics Seminar</a>.</li>
<li><strong>[Apr. 2023]</strong> I will serve as an Area Chair of <a href="https://www.auai.org/uai2023/">UAI 2023</a> and <a href="https://bmvc2023.rog/">BMVC 2023</a>.</li>
<li><strong>[Mar. 2023]</strong> I will participate in the <a href="https://cvpr2023.thecvf.com/Conferences/2023/CallForDoctoralConsortium">CVPR 2023 Doctoral Consortium</a> with a travel award. </li>
<li><strong>[Mar. 2023]</strong> I gave a talk on continual learning at <a href="https://vigr.cs.columbia.edu/vigr_seminar.html">Columbia VIGR Seminar</a>. </li>
<li><strong>[Mar. 2023]</strong> I gave a talk on few-shot learning at EPFL. </li>
<li><strong>[Feb. 2023]</strong> Two papers about <a href="./#publications">continual learning</a> are accepted to <a href="http://cvpr2023.thecvf.com/">CVPR 2023</a>.</li>
  
<li> <a href="javascript:toggle_vis('newsmore')">Show more</a> </li>
<div id="newsmore" style="display:none"> 
  <li><strong>[Dec. 2022]</strong> <a href="https://www.bmvc2023.org">BMVC 2023</a> will be held in Aberdeen, UK, and I will serve as the Website Chair.</li>
  <li><strong>[Nov. 2022]</strong> Our paper about <a href="https://pure.mpg.de/rest/items/item_3478882_1/component/file_3478883/content">class-incremental learning</a> is accepted to <a href="https://aaai.org/Conferences/AAAI-23/">AAAI 2023</a>.</li>
  <li><strong>[Oct. 2022]</strong> I am recognized as a top reviewer for <a href="https://neurips.cc/Conferences/2022/ProgramCommittee">NeurIPS 2022</a>.</li>
  <li><strong>[Aug. 2022]</strong> I will serve as an area chair of <a href="https://aistats.org/aistats2023/">AISTATS 2023</a>.</li>
  <li><strong>[Jun. 2022]</strong> I will serve as a student mentor of <a href="https://sites.google.com/view/cvpr-academy/">the CVPR Academy</a> at <a href="http://cvpr2022.thecvf.com/">CVPR 2022</a>.</li>
  <li><strong>[Jun. 2022]</strong> I will serve as a website chair of <a href="https://bmvc2022.org/people/organisers/">BMVC 2022</a>, along with <a href="https://yashbhalgat.github.io/">Yash Bhalgat</a>.</li>
  <li><strong>[Sep. 2021]</strong> Our paper about <a href="https://openreview.net/pdf?id=BfPzZSype5M">class-incremental learning</a> is accepted to <a href="https://neurips.cc/Conferences/2021">NeurIPS 2021</a>.</li>
  <li><strong>[Mar. 2021]</strong> Our paper about <a href="https://arxiv.org/pdf/2010.05063.pdf">class-incremental learning</a> is accepted to <a href="http://cvpr2021.thecvf.com/">CVPR 2021</a>.</li>
  <li><strong>[Jul. 2020]</strong> Our paper about <a href="https://link.springer.com/content/pdf/10.1007%2F978-3-030-58517-4_24.pdf">few-shot learning</a> is accepted to <a href="https://eccv2020.eu/">ECCV 2020</a>.</li>
  <li><strong>[Feb. 2020]</strong> Our paper about <a href="https://arxiv.org/pdf/2002.10211.pdf">class-incremental learning</a> is accepted to <a href="http://cvpr2020.thecvf.com/">CVPR 2020</a>.</li>
  <li><strong>[Feb. 2020]</strong> We will host the <a href="https://www.acmmmasia.org/2020/committee.html">ACM Multimedia Asia 2020</a> conference in Singapore!</li>
  <li><strong>[Sep. 2019]</strong> Our paper about <a href="https://papers.nips.cc/paper/2019/file/bf25356fd2a6e038f1a3a59c26687e80-Paper.pdf">few-shot learning</a> is accepted to <a href="https://nips.cc/Conferences/2019">NeurIPS 2019</a>.</li>
  <li><strong>[Mar. 2019]</strong> Our paper about <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Sun_Meta-Transfer_Learning_for_Few-Shot_Learning_CVPR_2019_paper.pdf">few-shot learning</a> is accepted to <a href="http://cvpr2019.thecvf.com/">CVPR 2019</a>.</li>
</div>

</ul> -->

<h1 id="publications"></h1>

<h2 style="margin: 30px 0px -15px;">Selected Publications and Preprints 
<!--<temp style="font-size:15px;">[</temp><a href="https://scholar.google.com/citations?user=Uf9GqRsAAAAJ" target="_blank" style="font-size:15px;">Google Scholar</a><temp style="font-size:15px;">]</temp><temp style="font-size:15px;">[</temp><a href="https://dblp.org/pid/12/10033-1.html" target="_blank" style="font-size:15px;">DBLP</a><temp style="font-size:15px;">]</temp>-->
</h2>

<div class="publications">
<ol class="bibliography">



<li>
<div class="pub-row">
  <div class="col-sm-3 abbr" style="position: relative;padding-right: 15px;padding-left: 15px;">
    <img src="./assets/teaser/EC-Flow.png" class="teaser img-fluid z-depth-1" style="width=100;height=40%" />
            <abbr class="badge">ICCV</abbr>
  </div>
  <div class="col-sm-9" style="position: relative;padding-right: 0px;padding-left: 10px;">
      <div class="title"><a href="https://ec-flow1.github.io/">EC-Flow: Enabling Versatile Robotic Manipulation from Action-Unlabeled Videos via Embodiment-Centric Flow</a></div>
      <div class="author">Yixiang Chen, Peiyan Li, Yan Huang, <strong>Jiabing Yang</strong>, Kehan Chen, Liang Wang</div>
      <div class="periodical"><em><i style="color:#1e90ff">ICCV 2025.</i></em>
      </div>
    <div class="links">
       
      <a href="https://ec-flow1.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Website</a>
          
       
      <a href="https://ec-flow1.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
      
       
      <a href="https://github.com/YixiangChen515/EC-Flow" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Code</a>
      
      
      
      
      
    </div>
  </div>
</div>
</li>

<br />


<!-- 
<li>
<div class="pub-row">
  <div class="col-sm-3 abbr" style="position: relative;padding-right: 15px;padding-left: 15px;">
    <img src="https://img.yliu.me/teaser/MTL_CVPR.png" class="teaser img-fluid z-depth-1">
            <abbr class="badge">CVPR</abbr>
  </div>
  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
      <div class="title"><a href="https://openaccess.thecvf.com/content_CVPR_2019/html/Sun_Meta-Transfer_Learning_for_Few-Shot_Learning_CVPR_2019_paper.html">Meta-Transfer Learning for Few-Shot Learning</a></div>
      <div class="author">Qianru Sun*, <strong>Yaoyao Liu*</strong>, Tat-Seng Chua, Bernt Schiele <br> (* Equal contribution)</div>
      <div class="periodical"><em>IEEE/CVF Conference on Computer Vision and Pattern Recognition <strong>(CVPR)</strong>, 2019.</em>
      </div>
    <div class="links">
      <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Sun_Meta-Transfer_Learning_for_Few-Shot_Learning_CVPR_2019_paper.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
      <a href="https://github.com/yaoyao-liu/meta-transfer-learning" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Code</a>
      <a href="https://lyy.mpi-inf.mpg.de/mtl/" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Project Page</a>
      <a href="https://dblp.uni-trier.de/rec/conf/cvpr/SunLCS19.html?view=bibtex" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">BibTex</a>
<br>
<strong> <a style="color:#e74d3c; font-weight:600" href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Uf9GqRsAAAAJ&citation_for_view=Uf9GqRsAAAAJ:bEWYMUwI8FkC"><i id="total_citation_mtl">800+</i><i style="color:#e74d3c; font-weight:600"> Citations â€¢ </i></a><a href="https://github.com/yaoyao-liu/meta-transfer-learning" target="_blank" rel="noopener"><i style="color:#e74d3c; font-weight:600" id="githubstars_mtl">600+</i><i style="color:#e74d3c; font-weight:600"> GitHub Stars</i></a> <a style="color:#e74d3c; font-weight:600" href="https://www.comp.nus.edu.sg/news/2019-cvpr-research/">â€¢ <i>Featured in NUS News</i></a></strong>
<br>
<strong><a style="color:#e74d3c; font-weight:600" href="https://scholar.google.com/citations?hl=en&view_op=list_hcore&venue=FXe-a9w0eycJ.2024&vq=en&cstart=60"><i>Top 100 Most Cited CVPR Papers over the Last Five Years</i></a></strong>
  <script>
  githubStars("yaoyao-liu/meta-transfer-learning", function(stars) {
  var startext = document.getElementById("githubstars_mtl");
        startext.innerHTML=stars;
  });
  </script>
  <script>
      $(document).ready(function () {
          
          var gsDataBaseUrl = 'https://raw.githubusercontent.com/yaoyao-liu/yaoyao-liu.github.io/'
          
          $.getJSON(gsDataBaseUrl + "google-scholar-stats/gs_data.json", function (data) {
              var totalCitation = data['publications']['Uf9GqRsAAAAJ:bEWYMUwI8FkC']['num_citations']
              document.getElementById('total_citation_mtl').innerHTML = totalCitation;
              var citationEles = document.getElementsByClassName('show_paper_citations')
              Array.prototype.forEach.call(citationEles, element => {
                  var paperId = element.getAttribute('data')
                  var numCitations = data['publications'][paperId]['num_citations']
                  element.innerHTML = '| Citations: ' + numCitations;
              });
          });
      })
  </script>
    </div>
  </div>
</div>
</li> -->

</ol>
</div>

<h1 id="contact"></h1>

<h2 style="margin: 30px 0px 10px;">Contact</h2>

<p><strong style="margin: 0 10px 0;">Address:</strong> <a href="https://map.baidu.com/poi/%E7%94%B5%E5%AD%90%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%A6(%E6%B8%85%E6%B0%B4%E6%B2%B3%E6%A0%A1%E5%8C%BA)/@11570395.127965013,3581537.6159742433,12.62z?uid=7001486489acc1e6a8f72947&amp;ugc_type=3&amp;ugc_ver=1&amp;device_ratio=2&amp;compat=1&amp;pcevaname=pc4.1&amp;querytype=detailConInfo&amp;da_src=shareurl">No. 2006 Xiyuan Avenue, High tech Zone, Chengdu City, Sichuan Province, China</a>
<br />

<strong style="margin: 0 10px 0;">Email:</strong> <email><a href="mailto:jiabingyang01@gmail.com">jiabingyang01@gmail.com</a></email>
<br />
<strong style="margin: 0 10px 0;">Phone:</strong> +86 18707064655</p>

<script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&amp;w=a&amp;t=tt&amp;d=6Rxz691fLksdEWBSvB3dNcZ2zUeCjGixNlaXP5xsIh8"></script>



      <br>

      

      

      <br>
      
      
      </section>
      <footer>
        <div style="font: 12px/1.2 Crimson Pro, serif">
        <p>
        <small>
        
        
        </small>
        </p>
        </div>
        <br>
        <br>
      </footer>
    </div>
    <script src="/assets/js/scale.fix.js"></script>
  </body>
</html>
